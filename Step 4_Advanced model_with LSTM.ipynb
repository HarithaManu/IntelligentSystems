{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDjuly9pr8NR"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import Counter\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, SpatialDropout1D\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import gc\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "import csv\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import nltk\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "#policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "#tf.keras.mixed_precision.set_global_policy(policy)\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
        "\n",
        "total_data_loaded = 0  # Variable to accumulate the total size of loaded data\n",
        "\n",
        "def load_data_with_tensorflow(filepath, chunksize=50000):\n",
        "    global total_data_loaded\n",
        "\n",
        "    dataset = tf.data.experimental.make_csv_dataset(\n",
        "        filepath,\n",
        "        batch_size=chunksize,\n",
        "        label_name='type',\n",
        "        select_columns=['content', 'type'],\n",
        "        num_epochs=1,\n",
        "        ignore_errors=True,\n",
        "        header=True\n",
        "    )\n",
        "\n",
        "    num_chunks = 0\n",
        "    for batch in tqdm(dataset.take(20), desc=\"Loading Data\"):\n",
        "        features_dict, labels = batch\n",
        "        features = features_dict['content']\n",
        "\n",
        "        # Convert to numpy and filter out empty rows\n",
        "        features_np = features.numpy()\n",
        "        labels_np = labels.numpy()\n",
        "        valid_indices = []\n",
        "\n",
        "        for i, (feature, label) in enumerate(zip(features_np, labels_np)):\n",
        "            # Check if both content and type columns are non-empty\n",
        "            if feature.strip() and label.strip():\n",
        "                valid_indices.append(i)\n",
        "\n",
        "        valid_features = features_np[valid_indices]\n",
        "        valid_labels = labels_np[valid_indices]\n",
        "\n",
        "        total_data_loaded += sum(len(feature) for feature in valid_features)\n",
        "        yield valid_features, valid_labels\n",
        "\n",
        "def preprocess_data(features, labels, stop_words, stemmer):\n",
        "    # Define a multiprocessing pool\n",
        "    pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())\n",
        "\n",
        "    # Preprocess each feature in parallel\n",
        "    processed_results = pool.starmap(process_text, [(text, stop_words, stemmer) for text in features])\n",
        "\n",
        "    # Close the pool\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "    processed_features = []\n",
        "    processed_labels = []\n",
        "    vocab_sizes = []  # List to store vocabulary sizes\n",
        "    vocab_sizes_after_filtering = []\n",
        "    urls_counts = []\n",
        "    dates_counts = []\n",
        "    numerics_counts = []\n",
        "    for result, label in zip(processed_results, labels):\n",
        "        if result:\n",
        "            processed_text, vocab_size, vocab_size_after_filtering, num_urls, num_dates, num_numerics = result\n",
        "            processed_features.append(processed_text)\n",
        "            processed_labels.append(classify_news_type(label.decode('utf-8')))  # Decode label to string\n",
        "            vocab_sizes.append(vocab_size)\n",
        "            vocab_sizes_after_filtering.append(vocab_size_after_filtering)\n",
        "            urls_counts.append(num_urls)\n",
        "            dates_counts.append(num_dates)\n",
        "            numerics_counts.append(num_numerics)\n",
        "\n",
        "    return processed_features, processed_labels, vocab_sizes, vocab_sizes_after_filtering, urls_counts, dates_counts, numerics_counts\n",
        "\n",
        "def process_text(text, stop_words, stemmer):\n",
        "    if not text.strip():  # Check if the text is empty or contains only whitespace\n",
        "        return '', 0, 0, 0, 0, 0  # Return empty strings and counts if the text is empty\n",
        "\n",
        "    # Decode the bytes-like object to a string\n",
        "    text = text.decode('utf-8')\n",
        "\n",
        "    # Count URLs in the content\n",
        "    num_urls = len(re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text))\n",
        "\n",
        "    # Count dates in the content\n",
        "    dates = re.findall(r'\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}', text) # Example date format: 01/01/2022\n",
        "    num_dates = len(dates)\n",
        "\n",
        "    # Count numeric values in the content\n",
        "    numerics = re.findall(r'\\b\\d+\\b', text)  # Extracts integers\n",
        "    num_numerics = len(numerics)\n",
        "\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    vocab_size = len(set(tokens))\n",
        "    filtered_tokens = [stemmer.stem(token) for token in tokens if token not in stop_words and token.isalpha()]\n",
        "    vocab_size_after_filtering = len(set(filtered_tokens))  # Unique tokens after filtering and stemming\n",
        "    processed_text = ' '.join(filtered_tokens)  # Processed text with stopwords removed and stemming applied\n",
        "    return processed_text, vocab_size, vocab_size_after_filtering, num_urls, num_dates, num_numerics\n",
        "\n",
        "def classify_news_type(news_type):\n",
        "    fake_types = ['fake', 'conspiracy', 'unreliable', 'satire', 'bias']\n",
        "    reliable_types = ['political', 'reliable']\n",
        "    if news_type.lower() in fake_types:\n",
        "        return 'Fake'\n",
        "    elif news_type.lower() in reliable_types:\n",
        "        return 'Reliable'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "def get_tokens_size_on_disk(tokens, filename):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Stemmed Tokens'])  # Write header\n",
        "        for token in tokens:\n",
        "            writer.writerow([token])  # Write each stemmed token to a separate row\n",
        "    return os.path.getsize(filename)\n",
        "\n",
        "def bytes_to_gb(size_in_bytes):\n",
        "    return size_in_bytes / (1024 ** 3)\n",
        "\n",
        "\n",
        "def build_lstm_model(max_features, embedding_dim, lstm_units, maxlen):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_features, embedding_dim, input_length=maxlen))\n",
        "    model.add(SpatialDropout1D(0.3))\n",
        "    model.add(LSTM(lstm_units//2, dropout=0.1, recurrent_dropout=0.1))\n",
        "    model.add(Dense(3, activation='softmax',))  # 3 classes: Fake, Reliable, Neutral\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# NEW: Add this helper function for optimized data loading\n",
        "def create_tf_dataset(features, labels, batch_size=512):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "    dataset = dataset.shuffle(buffer_size=10000)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "def main():\n",
        "    filepath = 'news_cleaned_2018_02_13.csv'  # Adjust the file path as needed\n",
        "    chunksize = 50000\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    dataset = load_data_with_tensorflow(filepath, chunksize=chunksize)\n",
        "\n",
        "    X_chunks = []\n",
        "    y_chunks = []\n",
        "    total_vocab_size = 0\n",
        "    total_vocab_size_after_filtering = 0\n",
        "    total_tokens_size_on_disk = 0\n",
        "    for chunk_num, (features, labels) in enumerate(dataset, start=1):\n",
        "        processed_data = preprocess_data(features, labels, stop_words, stemmer)\n",
        "        processed_features, processed_labels, vocab_sizes, vocab_sizes_after_filtering, urls_count, dates_count, numerics_count = processed_data\n",
        "\n",
        "        if not processed_features:\n",
        "            print(f\"No features loaded in chunk {chunk_num}.\")\n",
        "        if not processed_labels:\n",
        "            print(f\"No labels loaded in chunk {chunk_num}.\")\n",
        "\n",
        "        # Calculate and print the size of tokens on disk\n",
        "        for text in processed_features:\n",
        "            tokens = text.split()\n",
        "            tokens_filename = \"tokens.csv\"\n",
        "            tokens_size_on_disk = get_tokens_size_on_disk(tokens, tokens_filename)\n",
        "            total_tokens_size_on_disk += tokens_size_on_disk\n",
        "\n",
        "        X_chunks.extend(processed_features)\n",
        "        y_chunks.extend(processed_labels)\n",
        "        total_vocab_size += sum(vocab_sizes)\n",
        "        total_vocab_size_after_filtering += sum(vocab_sizes_after_filtering)\n",
        "\n",
        "    print(f\"Total Vocab Size: {total_vocab_size}\")\n",
        "    print(f\"Total Vocab Size after stemming: {total_vocab_size_after_filtering}\")\n",
        "    print(f\"Total data loaded: {total_data_loaded / (1024 ** 3):.6f} GB\")\n",
        "    print(f\"Total size of all tokens on disk: {bytes_to_gb(total_tokens_size_on_disk):.6f} GB\")\n",
        "\n",
        "    if not X_chunks or not y_chunks:\n",
        "        print(\"No data loaded. Please check the dataset or adjust parameters.\")\n",
        "        return\n",
        "\n",
        "    X = X_chunks\n",
        "    y = y_chunks\n",
        "\n",
        "    # Encode labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_encoded = label_encoder.fit_transform(y)\n",
        "    y_categorical = to_categorical(y_encoded)\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "    # TF-IDF Vectorization\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train).toarray()\n",
        "    X_test_tfidf = tfidf_vectorizer.transform(X_test).toarray()\n",
        "\n",
        "    # Tokenization for LSTM\n",
        "    max_features = 5000\n",
        "    tokenizer = Tokenizer(num_words=max_features, oov_token=\"<OOV>\")\n",
        "    tokenizer.fit_on_texts(X_train)\n",
        "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "    X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "    # Clip any values that might exceed max_features\n",
        "    X_train_seq = [[min(token, max_features-1) for token in seq] for seq in X_train_seq]  # NEW\n",
        "    X_test_seq = [[min(token, max_features-1) for token in seq] for seq in X_test_seq]  # NEW\n",
        "\n",
        "    # Padding sequences\n",
        "    maxlen = 200\n",
        "    X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen)\n",
        "    X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen)\n",
        "\n",
        "\n",
        "\n",
        "    # Build LSTM model\n",
        "    embedding_dim = 128\n",
        "    lstm_units = 64\n",
        "    model = build_lstm_model(max_features, embedding_dim, lstm_units, maxlen)\n",
        "\n",
        "    # NEW: Replace the model.fit() section with this optimized version:\n",
        "\n",
        "    # Create optimized datasets\n",
        "    train_dataset = create_tf_dataset(X_train_pad, y_train, batch_size=512)\n",
        "    val_dataset = create_tf_dataset(X_test_pad, y_test, batch_size=512)\n",
        "\n",
        "    # Add early stopping\n",
        "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=2,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "\n",
        "        epochs=5,\n",
        "\n",
        "        validation_data=val_dataset,\n",
        "        callbacks=[early_stop],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    y_pred = model.predict(X_test_pad)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_test_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Print classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test_classes, y_pred_classes, target_names=label_encoder.classes_))\n",
        "\n",
        "    # Print individual metrics\n",
        "    accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
        "    precision = precision_score(y_test_classes, y_pred_classes, average='weighted')\n",
        "    recall = recall_score(y_test_classes, y_pred_classes, average='weighted')\n",
        "    f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')\n",
        "\n",
        "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    }
  ]
}